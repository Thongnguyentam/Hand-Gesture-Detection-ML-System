# 6. Monitoring with Prometheus and Grafana

This guide explains how to set up our comprehensive monitoring stack using **Prometheus** for collecting time-series metrics, **Alertmanager** for handling alerts, and **Grafana** for visualizing data.

We deploy our monitoring stack in a `monitoring` namespace and expose all services through Nginx Ingress.

---

## 1. Prerequisites

Our setup requires:
- Running Kubernetes cluster with NGINX Ingress Controller
- Helm 3.x
- `kubectl` configured for cluster access

---

## 2. Setup Architecture

Our monitoring stack uses the **kube-prometheus-stack** Helm chart for:

- **Unified Configuration**: Single `values.yml` for all components
- **Dependency Management**: Automatic component dependency handling
- **Version Compatibility**: Ensures component compatibility
- **Simplified Updates**: Single command updates

### 2.2 Deployment Steps

#### Automated Setup with Our Script

We've created a comprehensive setup script that handles the entire deployment process:

```bash
# Show available options
./scripts/setup-prometheus.sh help

# Install Prometheus stack (recommended)
./scripts/setup-prometheus.sh install

# Clean up existing installation
./scripts/setup-prometheus.sh cleanup

# Reinstall (cleanup + install)
./scripts/setup-prometheus.sh reinstall

# Verify installation
./scripts/setup-prometheus.sh verify

# Show connection information
./scripts/setup-prometheus.sh info
```

#### Manual Step-by-Step Deployment

If you prefer manual deployment, here are the individual steps our script performs:

```bash
# 1. Add Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 2. Create namespace
kubectl create namespace monitoring

# 3. Deploy stack
helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values helm-charts/prometheus/values.yml \
  --timeout 10m \
  --wait

# 4. Verify deployment
kubectl get pods,svc,ingress -n monitoring

# 5. Get Grafana credentials
kubectl get secret prometheus-grafana -n monitoring -o jsonpath='{.data.admin-password}' | base64 --decode; echo
```

#### Cleanup Commands

```bash
# Using our script (recommended)
./scripts/setup-prometheus.sh cleanup

# Manual cleanup
helm uninstall prometheus -n monitoring
kubectl delete namespace monitoring
```

### 2.3 Architecture Overview

<div align="center">
  <img src="../images/prometheus_stack.png" alt="Prometheus Architecture 
  Overview" width="800">
  <p><em>Prometheus Architecture Overview</em></p>
</div>

Our monitoring stack consists of these key layers:

**üåê External Access**
```
Users ‚Üí NGINX Ingress Controller ‚Üí Monitoring Components
‚îú‚îÄ‚îÄ grafana.34.63.222.25.nip.io ‚Üí Grafana (Port 3000)
‚îú‚îÄ‚îÄ prometheus.34.63.222.25.nip.io ‚Üí Prometheus (Port 9090)  
‚îî‚îÄ‚îÄ alertmanager.34.63.222.25.nip.io ‚Üí Alertmanager (Port 9093)
```

**üìä Monitoring Namespace Components**
| Component | Type | Port | Purpose |
|-----------|------|------|---------|
| Prometheus Server | StatefulSet | 9090 | Metrics collection & storage |
| Grafana | Deployment | 3000 | Visualization dashboards |
| Alertmanager | StatefulSet | 9093 | Alert routing & grouping |
| Prometheus Operator | Deployment | 8080 | CRD management & config generation |
| Kube State Metrics | Deployment | 8080 | K8s object state metrics |
| Node Exporter | DaemonSet | 9100 | Node-level system metrics |

**üîß Control Plane Monitoring (All Enabled)**
- **K8s API Server** (6443/HTTPS): Cluster performance metrics
- **Kubelet + cAdvisor** (10250/HTTPS): Container & node metrics  
- **Etcd** (2381/HTTP): Database performance
- **CoreDNS** (9153/HTTP): DNS query metrics
- **Kube Proxy** (10249/HTTP): Network proxy stats
- **Controller Manager & Scheduler** (Dynamic ports): Control loop metrics

**üìà Data Flow**
1. **Service Discovery**: Prometheus Operator watches ServiceMonitor CRDs ‚Üí Generates configs
2. **Metrics Collection**: Prometheus scrapes all targets (nodes, control plane, apps)
3. **Storage**: Metrics stored in time-series database with retention policies
4. **Visualization**: Grafana queries Prometheus ‚Üí Renders dashboards
5. **Alerting**: Prometheus evaluates rules ‚Üí Sends alerts to Alertmanager

**Key Point**: Prometheus scrapes **both** K8s API Server (performance metrics) AND Kube State Metrics (object states) for complete visibility.

### 2.4 ServiceMonitor Configuration

Our applications use ServiceMonitor CRDs for automatic discovery:

```yaml
# helm-charts/asl/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {{ .Release.Name }}-servicemonitor
  namespace: {{ .Release.Namespace }}
  labels:
    release: prometheus  # Required for Prometheus Operator discovery
spec:
  namespaceSelector:
    matchNames:
      - {{ .Release.Namespace }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
```

### 2.5 Configuration Management

#### **Helm Values Structure**
```yaml
# helm-charts/prometheus/values.yml
prometheus:
  enabled: true
  ingress:
    enabled: true
    hosts:
      - prometheus.34.63.222.25.nip.io

grafana:
  enabled: true
  defaultDashboardsEnabled: true
  ingress:
    enabled: true
    hosts:
      - grafana.34.63.222.25.nip.io

alertmanager:
  enabled: true
  ingress:
    enabled: true
    hosts:
      - alertmanager.34.63.222.25.nip.io

# Control Plane Monitoring (All Enabled)
kubeApiServer:
  enabled: true
kubelet:
  enabled: true
kubeControllerManager:
  enabled: true
kubeScheduler:
  enabled: true
kubeProxy:
  enabled: true
kubeEtcd:
  enabled: true
coreDns:
  enabled: true
```

---

## 3. Application Monitoring

### How ServiceMonitors Work

1. **Discovery**: Prometheus Operator watches for ServiceMonitor CRDs with label `release: prometheus`
2. **Configuration**: Automatically generates Prometheus scrape configs
3. **Scraping**: Prometheus contacts Service ClusterIP on specified port/path
4. **Routing**: kube-proxy forwards requests to backend pods

### Verification Commands

```bash
# Use our verification script (recommended)
./scripts/setup-prometheus.sh verify

# Manual verification commands:

# Check ServiceMonitors
kubectl get servicemonitors -A

# Check PrometheusRules
kubectl get prometheusrules -A

# Verify scraping targets in Prometheus UI
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# Visit http://localhost:9090/targets

# Test if our app is being scraped
up{job="<app-name>-servicemonitor"}

# Check Grafana health
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
curl -s "http://localhost:3000/api/health"

# Check Alertmanager status
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093
curl -s "http://localhost:9093/api/v2/status"
```

---

## 4. Access URLs

```bash
# Get connection information (recommended)
./scripts/setup-prometheus.sh info

# Get Grafana admin password manually
kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 --decode; echo

# Port forward for local access
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093
```

Our monitoring stack is accessible via:

- **Grafana**: http://grafana.34.63.222.25.nip.io (admin/[password from script])
- **Prometheus**: http://prometheus.34.63.222.25.nip.io
- **Alertmanager**: http://alertmanager.34.63.222.25.nip.io

---

## 5. Essential PromQL Queries

### Basic Health Checks
```promql
# Check if our app is being scraped
up{job="hand-gesture-servicemonitor"}

# Request rate per second
sum(rate(http_requests_total{job="hand-gesture-servicemonitor"}[1m]))

# Error rate percentage
sum(rate(http_requests_total{job="hand-gesture-servicemonitor",status!~"2.."}[5m])) /
sum(rate(http_requests_total{job="hand-gesture-servicemonitor"}[5m])) * 100
```

### Performance Metrics
```promql
# 95th percentile response time
histogram_quantile(0.95, 
  sum by (le)(rate(http_request_duration_seconds_bucket{job="hand-gesture-servicemonitor"}[5m]))
)

# CPU usage
rate(process_cpu_seconds_total{job="hand-gesture-servicemonitor"}[1m])

# Memory usage
process_resident_memory_bytes{job="hand-gesture-servicemonitor"}
```

### Traffic Analysis
```promql
# Daily request volume
sum(increase(http_requests_total{job="hand-gesture-servicemonitor"}[24h]))

# Requests by endpoint
sum by (handler)(rate(http_requests_total{job="hand-gesture-servicemonitor"}[5m]))
```

---

## 6. PromQL Functions Reference

| **Function** | **Purpose** | **Example** |
|-------------|-------------|-------------|
| `rate()` | Per-second rate over time window | `rate(http_requests_total[5m])` |
| `sum()` | Aggregate current values | `sum(memory_usage_bytes)` |
| `increase()` | Total increase over time range | `increase(requests_total[1h])` |
| `histogram_quantile()` | Calculate percentiles | `histogram_quantile(0.95, buckets)` |