# 6. Monitoring with Prometheus and Grafana

This guide explains how to set up a comprehensive monitoring stack for your Kubernetes cluster using **Prometheus** for collecting time-series metrics, **Alertmanager** for handling alerts, and **Grafana** for visualizing data in powerful dashboards.

We will deploy the monitoring stack in a `monitoring` namespace and expose all services through Nginx Ingress for easy web access.

---

## 1. Prerequisites

Ensure you have:
- A running Kubernetes cluster with Nginx Ingress Controller installed
- Helm 3.x installed
- `kubectl` configured to access your cluster

---

## 2. Setup Monitoring Stack

### Create the Namespace
All monitoring components will live in their own namespace for better organization and resource management.
```bash
kubectl create namespace monitoring
```

### Add the Prometheus Helm Repository
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

### Install the Monitoring Stack with Ingress
We'll use a custom values file to enable Nginx Ingress for all monitoring services.

```bash
# Install with custom values that enable ingress
helm install prometheus prometheus-community/kube-prometheus-stack \
  -n monitoring \
  -f helm-charts/prometheus/values.yml
```

This installs:
- **Prometheus** - Metrics collection and storage
- **Grafana** - Visualization dashboards  
- **Alertmanager** - Alert handling and routing
- **Node Exporter** - Node-level metrics
- **Kube State Metrics** - Kubernetes object metrics

---

## 3. Accessing the Monitoring UIs

After deployment, you can access the monitoring services via these URLs:

### Grafana Dashboard
- **URL**: `http://grafana.34.63.222.25.nip.io`
- **Username**: `admin`
- **Password**: `prom-operator` (as configured in values.yaml)

Grafana comes with pre-built dashboards for:
- Kubernetes cluster overview
- Node resource usage
- Pod metrics
- Application performance

### Prometheus Query Interface
- **URL**: `http://prometheus.34.63.222.25.nip.io`
- Use this to write custom PromQL queries and explore metrics

### Alertmanager Console
- **URL**: `http://alertmanager.34.63.222.25.nip.io`
- Configure alert routing and notification channels

---

## 4. Monitoring Your Application

To monitor your hand gesture detection application, you need to:

### A. Instrument Your Application Code

Add Prometheus metrics to your FastAPI application:

1. **Add dependency to `pyproject.toml`:**
   ```toml
   dependencies = [
       # ... existing dependencies
       "prometheus-fastapi-instrumentator",
   ]
   ```

2. **Update `main.py`:**
   ```python
   from fastapi import FastAPI
   from prometheus_fastapi_instrumentator import Instrumentator

   app = FastAPI()

   # Add Prometheus metrics endpoint
   Instrumentator().instrument(app).expose(app)

   # ... your existing routes and logic ...
   ```

### B. Update Helm Chart for Metrics Scraping

Edit your application's deployment to add Prometheus scrape annotations:

```yaml
# In helm-charts/asl/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
  labels:
    app: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replicaCount }}
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "30000"
      labels:
        app: {{ .Release.Name }}
    spec:
      # ... rest of your deployment spec
```

These annotations tell Prometheus to:
✅ Scrape this pod (prometheus.io/scrape: "true")
✅ On port 30000 (prometheus.io/port)
✅ At the /metrics endpoint (prometheus.io/path)

### C. Redeploy Your Application

After making these changes:
```bash
# Rebuild and push your Docker image
docker build -t thongnguyen0101/hand-gesture-detection:latest .
docker push thongnguyen0101/hand-gesture-detection:latest

# Upgrade your Helm deployment
helm upgrade asl ./helm-charts/asl -n default
```

---

## 5. Creating Custom Dashboards

### In Grafana:
1. Navigate to `http://grafana.34.63.222.25.nip.io`
2. Login with `admin` / `prom-operator`
3. Go to **Dashboards** → **New** → **New Dashboard**
4. Add panels with PromQL queries like:
   - `rate(http_requests_total[5m])` - Request rate
   - `histogram_quantile(0.95, http_request_duration_seconds_bucket)` - 95th percentile latency
   - `up{job="asl"}` - Service availability

### Common Metrics to Monitor:
- **HTTP Request Rate**: `rate(http_requests_total[5m])`
- **Response Time**: `http_request_duration_seconds`
- **Error Rate**: `rate(http_requests_total{status=~"5.."}[5m])`
- **CPU Usage**: `rate(container_cpu_usage_seconds_total[5m])`
- **Memory Usage**: `container_memory_usage_bytes`

---

## 6. Setting Up Alerts

### Create Alert Rules
Create a file `prometheus-alerts.yaml`:
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: asl-alerts
  namespace: monitoring
spec:
  groups:
  - name: asl.rules
    rules:
    - alert: ASLServiceDown
      expr: up{job="asl"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "ASL service is down"
        description: "ASL service has been down for more than 1 minute"
    
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value }} errors per second"
```

Apply the alert rules:
```bash
kubectl apply -f prometheus-alerts.yaml
```

---

## 7. Maintenance Commands

### Check Monitoring Stack Status
```bash
# Check all monitoring pods
kubectl get pods -n monitoring

# Check services
kubectl get svc -n monitoring

# Check ingress
kubectl get ingress -n monitoring
```

### Upgrade Monitoring Stack
```bash
helm upgrade prometheus prometheus-community/kube-prometheus-stack \
  -n monitoring \
  -f helm-charts/prometheus/values.yaml
```

### Uninstall Monitoring Stack
```bash
helm uninstall prometheus -n monitoring
kubectl delete namespace monitoring
```

---

## 8. Troubleshooting

### Common Issues:

**Grafana Login Issues:**
```bash
# Get current admin password
kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 --decode; echo
```

**Prometheus Not Scraping Application:**
- Verify deployment annotations are correct
- Check if `/metrics` endpoint is accessible
- Ensure service discovery is working

**Ingress Not Working:**
```bash
# Check ingress controller
kubectl get pods -n ingress-nginx

# Verify ingress resources
kubectl describe ingress -n monitoring
```

This monitoring setup provides comprehensive observability for your Kubernetes cluster and applications, with easy web access through Nginx Ingress.

Get the current service monitors in monitoring 
```bash
kubectl -n monitoring get servicemonitors
```

What the ServiceMonitor actually does
1. It describes, in Kubernetes-native form, how Prometheus should reach your service:
- Which namespace(s) to look in (namespaceSelector).
- Which Service(s) to target (selector.matchLabels).
- On which port/path/interval to scrape (endpoints).

The Prometheus Operator watches for ServiceMonitor objects that carry the label release: `prometheus` (or whatever you configured in the kube-prometheus-stack chart).
- When it sees one, it automatically rewrites Prometheus’s internal configuration so that a new scrape job is created—no one touches prometheus.yml by hand.

3. At scrape time Prometheus contacts the Kubernetes Service’s ClusterIP on the named port (http = 30000) and path (/metrics).
- kube-proxy forwards the request to whichever pod(s) are behind the Service, so you get metrics even while pods scale, roll, or get rescheduled.


Cheat-sheet for reading PromQL
- Selector {job="hand-gesture-servicemonitor"} limits the query to the series produced by your ServiceMonitor.
- rate(metric[5m]) converts a monotonically increasing counter into a per-second rate, averaged over the sliding window inside [...].
- sum( … ) collapses multiple time-series into one by adding them.
- histogram_quantile() computes percentiles from Prometheus’ native histogram buckets.
- increase(counter[24h]) returns the absolute increment of a counter over the period (good for “how many” questions).


`sum()`:	Aggregation.	Adds up current values (e.g., sum of all memory used now)
`increase()`:	Rate of change over time.	Calculates the difference over a time range (e.g., how many requests in last 5 minutes)


1. Are your pods being scraped?
```PromQL
up{job="hand-gesture-servicemonitor"}
```
- If this returns 1, it means your pod is being successfully scraped.
- If it returns 0, Prometheus cannot scrape your pod (e.g., due to network, relabeling, target misconfiguration, etc.).


2. "How many requests per second are we getting?"
```pgsql
sum(rate(http_requests_total{job="hand-gesture-servicemonitor"}[1m]))
```
Calculates the pe

3. Which endpoints are active and their response statuses.
```pgsql
sum by (handler, status)(
  rate(http_requests_total{job="hand-gesture-servicemonitor"}[1m])
)
```
- What it does: Same as above, but breaks it down by:
  - handler: API endpoint or route.
  - status: HTTP response code.

4. Percentage of failed requests over total.
```psql
sum(rate(http_requests_total{job="hand-gesture-servicemonitor",status!~"2.."}[5m]))
/
sum(rate(http_requests_total{job="hand-gesture-servicemonitor"}[5m]))
```
What it does: Calculates error rate over 5 minutes:
- Numerator: non-2xx (i.e., errors).
- Denominator: total requests.

5. Measures latency of slowest 5% of requests.
```psql
histogram_quantile(
  0.95,
  sum by (le)(
    rate(http_request_duration_seconds_bucket{job="hand-gesture-servicemonitor"}[5m])
  )
)
```
Computes the 95th percentile response time.

- http_request_duration_seconds_bucket is a histogram metric.
- le = "less than or equal to" bucket values.

6. Tracks CPU usage over time.
```psql
rate(process_cpu_seconds_total{job="hand-gesture"}[1m])
```
- CPU usage in seconds per second (i.e., CPU load).

7. Detect memory bloat or leaks.

```psql
process_resident_memory_bytes{job="hand-gesture"}
```
- Shows current memory usage in bytes (resident set size).

8. Number of Python garbage collections (gen 0) in the last 5 minutes.
```psql
increase(python_gc_collections_total{job="hand-gesture-servicemonitor",generation="0"}[5m])
```

9. Watch for resource leaks or limits being approached.
```psql
process_open_fds{job="hand-gesture-servicemonitor"}
```
- Current number of open file descriptors

10. Daily traffic volume analysis.
```psql
sum(increase(http_requests_total{job="hand-gesture-servicemonitor"}[24h]))
```
- Total number of HTTP requests in the last 24 hours.

11. See past http requests 
```psql
http_requests_total{job="hand-gesture"}
http_requests_total{job="hand-gesture", handler!="/metrics"} # exclude /metrics
```