# Mastering Distributed Tracing with Jaeger: A Complete Guide for FastAPI Applications

*From zero to hero: Building production-ready distributed tracing for microservices*

---

## What We'll Learn

In this comprehensive guide, we'll discover how to implement distributed tracing in FastAPI applications using Jaeger. Whether we're debugging performance issues or building observability into microservices, this tutorial will take us from basic concepts to production deployment.

**By the end of this article, we'll be able to:**
- Understand distributed tracing fundamentals and why it matters
- Set up Jaeger for FastAPI applications
- Implement custom tracing strategies for business insights
- Deploy and configure Jaeger in Kubernetes
- Debug common tracing issues like a pro

---

## Table of Contents

1. [The Problem: Why Distributed Tracing Matters](#the-problem)
2. [Meet Jaeger: Our Observability Solution](#meet-jaeger)
3. [Understanding Jaeger's Architecture](#architecture)
4. [Implementation: From Code to Production](#implementation)
5. [Advanced Tracing Strategies](#advanced-strategies)
6. [Deployment Guide: Kubernetes & Helm](#deployment)
7. [Monitoring and Analysis](#monitoring)
8. [Troubleshooting: Debug Like a Pro](#troubleshooting)
9. [Best Practices for Production](#best-practices)

---

## The Problem: Why Distributed Tracing Matters {#the-problem}

Picture this: Our hand-gesture detection API is running slow. Users are complaining about response times, but we have no idea where the bottleneck is.

**Without distributed tracing, we're flying blind:**
```
"The API is slow, but I don't know if it's the ML model, 
database, or network causing the issue."
```

**With distributed tracing, we have X-ray vision:**
```
"The ML model inference takes 150ms, but the database 
query takes 2 seconds. I need to optimize the database."
```

### The Microservices Observability Challenge

Modern applications are distributed across multiple services. A single user request might touch:
- API Gateway
- Authentication Service
- Image Processing Service
- ML Model Service
- Database
- Cache Layer

When something goes wrong, traditional logging falls short. We need to see the **entire journey** of a request across all services.

---

## Meet Jaeger: Our Observability Solution {#meet-jaeger}

Jaeger is like a **flight recorder for microservices**. It captures the journey of every request through our system, showing us:

- **Where time is spent** - Which operations are slow?
- **Where errors occur** - Which component failed?
- **How services interact** - What calls what?
- **Performance trends** - Are things getting slower over time?

### What Makes Jaeger Special?

**Production-Ready**: Used by Uber, Red Hat, and thousands of companies
**Easy Integration**: Works with OpenTelemetry standards
**Rich UI**: Beautiful web interface for exploring traces
**Vendor Neutral**: No lock-in, works with any observability stack

---

## Understanding Jaeger Architecture {#architecture}

### The Four Pillars of Jaeger

Think of Jaeger as a **postal system for traces**:

#### 1. **Jaeger Agent** - The Local Post Office
```
What it does:
- Receives traces locally and buffers them
- Forwards traces to collector
- Runs as DaemonSet (one per node)

Key Ports:
- 6831: UDP Thrift (compact)
- 5778: HTTP (configuration)
```

**Note**: Our current setup bypasses the agent for simplicity.

#### 2. **Jaeger Collector** - The Central Processing Hub
```
What it does:
- Receives traces from agents
- Validates and processes trace data
- Batches traces for efficient storage
- Handles multiple input formats

Key Ports:
- 14250: gRPC (from agents)
- 14268: HTTP (direct from apps)
- 4317: OTLP gRPC (OpenTelemetry) ← OUR APP USES THIS
- 4318: OTLP HTTP (OpenTelemetry)

Our Implementation:
- Endpoint: jaeger-collector.tracing.svc.cluster.local:4317
- Protocol: OTLP gRPC (modern, efficient)
- Direct connection (bypasses agent)
```

**Why it exists**: Centralized processing ensures data consistency and efficient storage.

#### 3. **Jaeger Query** - The Search Engine
```
What it does:
- Provides the web UI for viewing traces
- Offers REST API for programmatic access
- Searches and retrieves traces from storage
- Serves the beautiful Jaeger dashboard

Key Ports:
- 16686: Web UI and API ← OUR ACCESS POINT
- 16687: gRPC API

Our Implementation:
- Internal: jaeger-query.tracing.svc.cluster.local:16686
- External: jaeger.34.63.222.25.nip.io (via Ingress)
- Access: Web browser to view traces
```

**Why it exists**: We need a way to search and visualize traces.

#### 4. **Storage Backend** - The Memory Bank
```
What it does:
- Stores trace data persistently
- Provides fast retrieval for queries
- Handles data retention policies
- Scales with trace volume
```

### Visual Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Your FastAPI  │───▶│  Jaeger Agent   │───▶│ Jaeger Collector│
│   Application   │    │  (Local Proxy)  │    │ (Data Processor)│
│   Port: 8000    │    │  Port: 6831     │    │  Port: 4317     │
│                 │    │  (UDP Thrift)   │    │  (OTLP gRPC)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                                        │
                                                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│     Users       │───▶│  Jaeger Query   │───▶│  Storage Backend│
│ (Jaeger UI)     │    │   (Search API)  │    │ (Memory/ES/etc) │
│ Port: 80/443    │    │  Port: 16686    │    │                 │
│ (via Ingress)   │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Implementation Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                    FastAPI Application                         │
│                                                                │
│  ┌─────────────────┐    ┌─────────────────┐                    │
│  │   main.py       │    │   tracer.py     │                    │
│  │                 │    │                 │                    │
│  │ • Health Check  │    │ • OTLPSpanExporter                   │ 
│  │ • Instrumentor  │    │ • BatchSpanProcessor                 │
│  │ • CORS/Sessions │    │ • Resource Config                    │
│  └─────────────────┘    └─────────────────┘                    │
│                                                                │
│  Environment Variables (config.py):                            │
│  • ENABLE_TRACING=true                                         │
│  • JAEGER_AGENT_HOST=jaeger-agent.tracing.svc.cluster.local    │
│  • OTEL_SERVICE_NAME=hand-gesture-api                          │
└────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼ OTLP gRPC
┌───────────────────────────────────────────────────────────────┐
│                    Jaeger Collector                           │
│                                                               │
│  Service: jaeger-collector.tracing.svc.cluster.local          │
│  Port: 4317 (OTLP gRPC) ← Our app sends traces here           │
│  Port: 4318 (OTLP HTTP)                                       │
│  Port: 14268 (Jaeger HTTP)                                    │
│  Port: 14250 (Jaeger gRPC from agents)                        │
│                                                               │
│  Batch Processing:                                            │
│  • max_queue_size: 2048                                       │
│  • schedule_delay_millis: 5000                                │
│  • max_export_batch_size: 512                                 │
│  • export_timeout_millis: 30000                               │
└───────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Storage Backend                              │
│                                                                 │
│  Type: Memory (Development)                                     │
│  • Fast access                                                  │
│  • No persistence                                               │
│  • Suitable for testing                                         │
└─────────────────────────────────────────────────────────────────┘
                                    ▲
                                    │
┌─────────────────────────────────────────────────────────────────┐
│                    Jaeger Query                                 │
│                                                                 │
│  Service: jaeger-query.tracing.svc.cluster.local                │
│  Port: 16686 (Web UI)                                           │
│  Port: 16687 (gRPC API)                                         │
│                                                                 │
│  Accessible via:                                                │
│  • Ingress: jaeger.34.63.222.25.nip.io                          │
│  • Internal: http://jaeger-query:16686                          │
└─────────────────────────────────────────────────────────────────┘
```
## Key terms 

1. **What is JaegerExporter?**
- JaegerExporter is a component provided by OpenTelemetry that sends trace data (spans) from your application to Jaeger using Jaeger’s native protocols, like Thrift over UDP or Thrift over HTTP.
- This was the traditional way to send traces to Jaeger before Jaeger added native support for OpenTelemetry Protocol (OTLP).
- Example use:

```python
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
JaegerExporter(agent_host_name='localhost', agent_port=6831)
```

- ⚠ Status: Now considered deprecated, because Jaeger’s collector natively supports OTLP, so using Jaeger’s Thrift exporter is unnecessary.

2. **What is OpenTelemetry Protocol (OTLP)?**

OpenTelemetry Protocol (OTLP) is the official, standard protocol designed by the OpenTelemetry project to transmit telemetry data (traces, metrics, logs) between applications, collectors, and backends. It defines:

- What the data looks like (format) — structured using Protobuf (efficient serialization format).
- How it’s sent (transport) — typically over gRPC or HTTP/Protobuf.

How OTLP works:
Your app → sends data → via OTLP exporter → to collector / backend

Transport:
- gRPC (default, efficient, supports streaming)
- HTTP/protobuf (alternative, easier to debug with tools like curl)

Your app can export traces like:
```python
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

exporter = OTLPSpanExporter(
    endpoint="http://localhost:4317",  # OTLP gRPC endpoint
    insecure=True
)
```

Jaeger, Tempo, or any OTLP-compatible collector will ingest this data.

3. **What is OTLPSpanExporter?**
- OTLPSpanExporter is the standard OpenTelemetry Protocol (OTLP) exporter.
- It sends traces in the OTLP format (usually over gRPC or HTTP) to any collector or backend that understands OTLP — including Jaeger, Tempo, Honeycomb, etc.
- It aligns with OpenTelemetry’s standardization efforts and is preferred for compatibility and future-proofing.
- Example use:
```python
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
OTLPSpanExporter(endpoint='http://localhost:4317', insecure=True)
```
- Key differences: OTLPSpanExporter has target to Jaeger, or any OTLP-compatible backend and transport through gRPC / HTTP Protobuf while JaegerExporter transport to Jaeger only through UDP / HTTP Thrift

### Key Implementation Details

**1. Direct OTLP Connection (Our Current Setup):**
Our application bypasses the Jaeger Agent and connects directly to the Jaeger Collector using OTLP gRPC on port 4317:

```python
# From tracer.py
otlp_exporter = OTLPSpanExporter(
    endpoint=f"{JAEGER_AGENT_HOST}:4317",
    insecure=True,
)
```

**Benefits of Our Current Approach:**
- **Simpler architecture** - One less component to manage
- **Lower latency** - Direct path to collector
- **Modern protocol** - Uses OTLP (industry standard)
- **Better error handling** - Direct connection errors are clearer
- **Resource efficient** - No agent overhead per node

**2. Conditional Tracing:**
Tracing is only enabled when `ENABLE_TRACING=true`:

**3. Comprehensive Instrumentation:**
Our setup includes automatic instrumentation for:
- FastAPI requests and responses
- HTTP requests via RequestsInstrumentor
- MongoDB operations via PymongoInstrumentor

### Configuration Reference

**Environment Variables (from config.py):**
```bash
# Tracing Control
ENABLE_TRACING=true                    # Enable/disable tracing
OTEL_SERVICE_NAME=hand-gesture-api     # Service identifier in traces

# Direct Collector Connection
JAEGER_COLLECTOR_HOST=jaeger-collector.tracing.svc.cluster.local
JAEGER_AGENT_PORT=6831                 # Not used in current setup
OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger-collector.tracing.svc.cluster.local:14268/api/traces

# Service Identity
JAEGER_HOSTNAME=hand-gesture-api       # Instance identifier
```

**Actual Connection (from tracer.py):**
```python
# Our app connects directly to collector via OTLP gRPC
endpoint = f"{JAEGER_COLLECTOR_HOST}:4317"
# Resolves to: http://jaeger-collector.tracing.svc.cluster.local:4317

# Batch processing configuration
BatchSpanProcessor(
    otlp_exporter,
    max_queue_size=2048,           # Buffer size
    schedule_delay_millis=5000,    # Export every 5 seconds
    max_export_batch_size=512,     # Traces per batch
    export_timeout_millis=30000,   # 30 second timeout
)
```

**Health Check Integration (from main.py):**
```python
@app.get("/health")
async def health_check():
    if tracer:
        with tracer.start_as_current_span("health_check") as span:
            span.set_attribute("health.status", "ok")
            return {"status": "ok", "tracing": "enabled"}
    return {"status": "ok", "tracing": "disabled"}
```

### OTLP: The Universal Language

**OpenTelemetry Protocol (OTLP)** is like **JSON for traces** - a standard format that everyone understands.

#### OTLP gRPC vs OTLP HTTP

| Feature | OTLP gRPC (Port 4317) | OTLP HTTP (Port 4318) |
|---------|----------------------|----------------------|
| **Performance** | Faster (binary) | Slower (JSON) |
| **Debugging** | Harder to debug | Human-readable |
| **Firewall** | May be blocked | HTTP-friendly |
| **Use Case** | Production | Development/Testing |

### Ingress Controller (Nginx)

The **Nginx Ingress Controller** routes external traffic to Jaeger UI:
- **External URL**: jaeger.34.63.222.25.nip.io
- **Internal Service**: jaeger-query:16686

### OpenTelemetry Collector: Do We Need It?

**OpenTelemetry Collector** is a universal translator for telemetry data that can receive, process, and route traces to multiple destinations.

**Our current setup is simpler and better because:**
- **Direct connection** - No extra components
- **Lower latency** - Fewer hops
- **Easier debugging** - Less complexity

**Consider OpenTelemetry Collector only if we need:**
- Multiple trace destinations (Jaeger + DataDog + Zipkin)
- Data transformation or filtering
- Vendor-neutral telemetry collection

**Recommendation**: Stick with our current direct connection approach.

---

## Direct Connection vs Jaeger Agent

```
┌─────────────────┐    ┌─────────────────┐
│   FastAPI App   │───▶│ Jaeger Collector│
│   Port: 8000    │    │   Port: 4317    │
│                 │    │   (OTLP gRPC)   │
└─────────────────┘    └─────────────────┘
```

**Benefits:**
- **Simpler** - Fewer components
- **Lower latency** - Direct path
- **Modern protocol** - OTLP standard
- **Easier debugging** - Less complexity

### Alternative: Via Jaeger Agent

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   FastAPI App   │───▶│  Jaeger Agent   │───▶│ Jaeger Collector│
│   Port: 8000    │    │   Port: 6831    │    │   Port: 14250   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

**Use Jaeger Agent only when:**
- **High volume** - 1000+ requests/second need local buffering
- **Legacy apps** - Using old Jaeger clients
- **Multiple apps per node** - Need shared agent
- **Network issues** - Unreliable connections

### Performance Comparison

| Aspect | Direct (Our Setup) | Via Agent |
|--------|-------------------|-----------|
| **Latency** | Lower | Higher |
| **Complexity** | Simple | Complex |
| **Protocol** | OTLP | Thrift |


---

## Storage Options Explained {#storage}

Choosing the right storage is like choosing the right database for our application. Each has trade-offs:

### 1. Memory Storage (Development)

```yaml
storage:
  type: memory
```

**Perfect for**: Development, testing, demos

**Pros**:
- **Lightning fast** - Everything in RAM
- **Zero setup** - No external dependencies
- **Simple** - Just works out of the box
- **Cost-effective** - No additional infrastructure

**Cons**:
- **Data loss** - Restart = lose everything
- **Limited capacity** - Constrained by available RAM
- **Single node** - No distribution
- **No persistence** - Not suitable for production

**Example Use Case**: 
*"We're developing a new feature and want to see traces immediately without setting up a database."*

### 2. Elasticsearch Storage (Production)

```yaml
storage:
  type: elasticsearch
  elasticsearch:
    host: elasticsearch-master
    port: 9200
    user: elastic
    password: changeme
    indexPrefix: jaeger
```

**Perfect for**: Production, large scale, analytics

**Pros**:
- **Persistent** - Data survives restarts
- **Searchable** - Rich query capabilities
- **Scalable** - Horizontal scaling
- **Analytics** - Built-in aggregations
- **Mature** - Battle-tested in production

**Cons**:
- **Complex** - Requires Elasticsearch expertise
- **Resource-heavy** - High CPU/memory usage
- **Operational overhead** - Monitoring, backups, updates
- **Cost** - Infrastructure and maintenance

**Example Use Case**: 
*"We have a production system with millions of traces and need powerful search capabilities."*

**Configuration Options**:

### Option 1: Use Existing Elasticsearch Cluster (Recommended)

If you already have an Elasticsearch cluster (e.g., from ELK stack in `logging` namespace), reuse it:

1. **Configure to use existing Elasticsearch**:
   ```yaml
   provisionDataStore:
     elasticsearch: false  # Don't provision new cluster
   
   storage:
     type: elasticsearch
     elasticsearch:
       host: elasticsearch-master.logging.svc.cluster.local
       port: 9200
       user: elastic
       existingSecret: elasticsearch-master-credentials
       existingSecretKey: password
       indexPrefix: jaeger
   ```

2. **Copy Elasticsearch credentials to tracing namespace**:
   ```bash
   # Copy the secret from logging to tracing namespace
   kubectl get secret elasticsearch-master-credentials -n logging -o yaml | \
     sed 's/namespace: logging/namespace: tracing/' | \
     kubectl apply -f -
   ```

3. **Deploy Jaeger**:
   ```bash
    helm upgrade --install jaeger jaegertracing/jaeger -n tracing -f helm-charts/jaeger/values.yaml
   ```

### Option 2: Provision Separate Elasticsearch Cluster

Only use this if you need isolation between logging and tracing:

1. **Enable Elasticsearch provisioning**
2. **Configure new cluster**

### Production Best Practices

**✅ Recommended: Shared Elasticsearch Cluster**
- **Resource efficiency** - One cluster serves both logging and tracing
- **Operational simplicity** - Manage one Elasticsearch cluster
- **Cost effective** - Reduce infrastructure overhead
- **Data correlation** - Easier to correlate logs and traces

**❌ Not Recommended: Separate Clusters**
- **Higher resource usage** - Multiple Elasticsearch clusters
- **Operational complexity** - More clusters to manage
- **Increased costs** - Duplicate infrastructure

**Important Notes**:
- **Index separation** - Jaeger uses `jaeger-*` indices, logs use `filebeat-*` or `logstash-*`
- **No conflicts** - Different index prefixes prevent data mixing
- **Shared resources** - Both systems benefit from larger cluster capacity
- **No changes needed to main.py** - Application code remains the same

### Understanding Configuration Parameters

**Index Prefix (`indexPrefix: jaeger`)**:
- Creates Elasticsearch indices with `jaeger-` prefix
- Example indices: `jaeger-span-2024-01-15`, `jaeger-service-2024-01-15`
- Separates trace data from log data (`filebeat-*`, `logstash-*`)

**Existing Secret Key (`existingSecretKey: password`)**:
- Tells Jaeger which field to read from the Kubernetes secret
- The Elasticsearch secret structure:
  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: elasticsearch-master-credentials
  data:
    username: ZWxhc3RpYw==  # base64 encoded "elastic"
    password: <base64-encoded-random-password>
  ```
- `existingSecretKey: password` means "read the `password` field from the secret"
- `existingSecretKey: username` would mean "read the `username` field"

### 3. Kafka Storage (Streaming)

```yaml
storage:
  type: kafka
  kafka:
    brokers: ["kafka:9092"]
    topic: jaeger_v1_test
```

**Perfect for**: Real-time processing, event streaming

**Pros**:
- **High throughput** - Millions of traces per second
- **Decoupled** - Separates ingestion from storage
- **Real-time** - Stream processing capabilities
- **Fault-tolerant** - Built-in replication

**Cons**:
- **Complex architecture** - Requires additional storage backend
- **Operational complexity** - Kafka cluster management
- **Not a storage** - Need another system for queries
- **Latency** - Additional hop in the pipeline

**Example Use Case**: 
*"We need to process traces in real-time and feed them to multiple systems."*

### 4. Cassandra Storage (Scale)

```yaml
storage:
  type: cassandra
  cassandra:
    host: cassandra
    port: 9042
```

**Perfect for**: Massive scale, multi-datacenter

**Pros**:
- **Massive scale** - Petabytes of data
- **Distributed** - Multi-datacenter replication
- **Always available** - No single point of failure
- **Linear scaling** - Add nodes = more capacity

**Cons**:
- **Complex** - Requires Cassandra expertise
- **Eventually consistent** - May see stale data
- **Operational overhead** - Complex maintenance
- **Overkill** - For most use cases

**Example Use Case**: 
*"We're Netflix and need to store traces from millions of microservices across multiple datacenters."*

### Storage Decision Matrix

| Scale | Persistence | Complexity | Recommendation |
|-------|-------------|------------|----------------|
| **Small** | Not needed | Low | **Memory** |
| **Medium** | Required | Medium | **Elasticsearch** |
| **Large** | Required | High | **Cassandra** |
| **Streaming** | External | Very High | **Kafka** |

---

## Implementation Guide {#implementation}

### Step 1: Application Setup

Our FastAPI application is already configured with:
- **OTLPSpanExporter** - Modern OTLP protocol
- **BatchSpanProcessor** - Efficient batch processing
- **Auto-instrumentation** - FastAPI, HTTP requests, MongoDB

### Step 2: Environment Configuration

Key environment variables:
```bash
ENABLE_TRACING=true
OTEL_SERVICE_NAME=hand-gesture-api
JAEGER_COLLECTOR_HOST=jaeger-collector.tracing.svc.cluster.local
```

---

## Custom Tracing Strategies {#custom-tracing}

### Why Add Custom Spans?

**Automatic instrumentation** gives us HTTP request traces, but **custom spans** give us business insights:

```python
# What automatic instrumentation shows:
POST /predict → 200 OK (500ms)

# What custom spans reveal:
POST /predict → 200 OK (500ms)
├── image_preprocessing (50ms)
├── landmark_detection (100ms)
├── model_inference (300ms)    ← The bottleneck!
└── response_formatting (50ms)
```

### The Art of Span Design

Think of spans as **chapters in a book** - each telling a complete story:

**Key principles:**
- **Business-level spans** - Main operations (e.g., "gesture_prediction")
- **Technical-level spans** - Implementation details (e.g., "image_preprocessing", "model_inference")
- **Rich attributes** - Add context like user_id, processing_time, confidence scores
- **Error handling** - Capture exceptions and set error status

### Real-World Example: ASL Prediction Service
---

## Deployment Instructions {#deployment}

### Option 1: Automated Setup 

```bash
# Run our setup script
./scripts/setup-jaeger.sh
```

This script:
1. Adds the Jaeger Helm repository
2. Creates the tracing namespace
3. Deploys Jaeger with our custom configuration
4. Sets up ingress for external access

### Option 2: Manual Setup

#### Step 1: Add Jaeger Helm Repository

```bash
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm repo update
```

#### Step 2: Create Namespace

```bash
kubectl create namespace tracing
```

#### Step 3: Deploy Jaeger

```bash
helm upgrade --install jaeger jaegertracing/jaeger \
  --namespace tracing \
  --values helm-charts/jaeger/values.yaml
```

#### Step 4: Verify Deployment

```bash
# Check all pods are running
kubectl get pods -n tracing

# Check services
kubectl get svc -n tracing

# Check ingress
kubectl get ingress -n tracing
```

---

## Monitoring and Analysis {#monitoring}

### Accessing Jaeger UI

**Production**: http://jaeger.34.63.222.25.nip.io
**Development**: http://localhost:16686 (with port forwarding)

### Understanding the Jaeger UI

#### 1. Search Interface

The search page is our **detective toolkit**:

```
Search Options:
- Service: hand-gesture-api
- Operation: asl_prediction
- Tags: user.id=john123
- Duration: >100ms
- Time Range: Last 1 hour
```

#### 2. Useful Queries

**Find slow predictions:**
```
service=hand-gesture-api operation=asl_prediction duration>500ms
```

**Find failed requests:**
```
service=hand-gesture-api error=true
```

**Find specific user's activity:**
```
service=hand-gesture-api prediction.user_id=user123
```

**Find low-confidence predictions:**
```
service=hand-gesture-api prediction.confidence<0.7
```

---

## Best Practices {#best-practices}

### 1. Sampling Strategy

**Don't trace everything** - it's expensive and overwhelming:

```yaml
# Development: Trace everything
sampling:
  type: probabilistic
  param: 1.0  # 100% sampling

# Production: Sample intelligently
sampling:
  type: probabilistic
  param: 0.1  # 10% sampling
```

### 2. Span Naming Conventions

**Use descriptive, consistent names:**

```python
# Good span names
"user_authentication"
"image_preprocessing"
"model_inference"
"database_query"

# Bad span names
"process"
"handle"
"do_stuff"
"function1"
```

### 3. Error Handling

**Capture errors properly:**

```python
try:
    result = risky_operation()
    span.set_attribute("operation.success", True)
except Exception as e:
    span.record_exception(e)
    span.set_status(Status(StatusCode.ERROR, str(e)))
    span.set_attribute("operation.success", False)
    raise
```

---

## Troubleshooting {#troubleshooting}

### Common Issues and Solutions

#### 1. "No traces appearing in Jaeger UI"

**Debug steps:**
```bash
# Check if our app is sending traces
kubectl logs -n model-serving deployment/hand-gesture-deployment | grep -i trace

# Check Jaeger agent logs
kubectl logs -n tracing daemonset/jaeger-agent

# Check Jaeger collector logs
kubectl logs -n tracing deployment/jaeger-collector
```

#### 2. "High latency in application"

**Solutions:**
```python
# Use async batch export
span_processor = BatchSpanProcessor(
    jaeger_exporter,
    schedule_delay_millis=5000,  # Export every 5 seconds
    max_export_batch_size=512    # Batch size
)
```

---

## Debugging Commands

### Basic Jaeger Operations
```shell
# Deploy/upgrade Jaeger with Elasticsearch
helm upgrade --install jaeger jaegertracing/jaeger --namespace tracing --values helm-charts/jaeger/values.yaml

# Check all pods are running
kubectl get pods -n tracing

# Check services
kubectl get svc -n tracing

# Check ingress
kubectl get ingress -n tracing
```

### Elasticsearch Specific Commands
```shell
# Check Elasticsearch pod status (in logging namespace)
kubectl get pods -n logging -l app=elasticsearch-master

# Check Elasticsearch logs
kubectl logs -n logging -l app=elasticsearch-master

# Port forward to Elasticsearch for direct access
kubectl port-forward -n logging svc/elasticsearch-master 9200:9200

# Check Elasticsearch health
curl -X GET "localhost:9200/_cluster/health?pretty"

# List all indices (including Jaeger)
curl -X GET "localhost:9200/_cat/indices?v"

# List only Jaeger indices
curl -X GET "localhost:9200/_cat/indices/jaeger*?v"

# Check Jaeger index mapping
curl -X GET "localhost:9200/jaeger-*/_mapping?pretty"

# Get Elasticsearch credentials
kubectl get secret elasticsearch-master-credentials -n logging -o jsonpath='{.data.username}' | base64 --decode; echo
kubectl get secret elasticsearch-master-credentials -n logging -o jsonpath='{.data.password}' | base64 --decode; echo
```

### Troubleshooting Elasticsearch Issues
```shell
# Check Elasticsearch cluster status
kubectl exec -n logging deployment/elasticsearch-master -- curl -X GET "localhost:9200/_cluster/health?pretty"

# Check available storage
kubectl exec -n logging deployment/elasticsearch-master -- df -h

# Check Elasticsearch configuration
kubectl exec -n logging deployment/elasticsearch-master -- cat /usr/share/elasticsearch/config/elasticsearch.yml

# Test connectivity from Jaeger pods
kubectl exec -n tracing deployment/jaeger-collector -- curl -X GET "http://elasticsearch-master.logging.svc.cluster.local:9200/_cluster/health?pretty"

# Copy Elasticsearch credentials to tracing namespace
kubectl get secret elasticsearch-master-credentials -n logging -o yaml | \
  sed 's/namespace: logging/namespace: tracing/' | \
  kubectl apply -f -
```

### Index Management
```shell
# Check index sizes
curl -X GET "localhost:9200/_cat/indices/jaeger*?v&s=store.size:desc"

# Delete old Jaeger indices (example: older than 30 days)
curl -X DELETE "localhost:9200/jaeger-span-$(date -d '30 days ago' +%Y-%m-%d)"

# Check index lifecycle policies
curl -X GET "localhost:9200/_ilm/policy"
```

- Restarts pods based on the current deployed spec in Kubernetes — no Helm interaction
```shell
kubectl rollout restart deployment/hand-gesture-deployment -n model-serving
```

- Uses Helm to upgrade or install a release named hand-gesture in the tracing namespace:
```shell
helm upgrade --install hand-gesture -f helm-charts/asl/ -n tracing
```

- See if Tracer is being used in the application
```shell
kubectl logs -n model-serving hand-gesture-deployment-5d5454bf94-4qrmx | head -30
```

Test the Application and Generate Traces
```shell
curl -s http://asl.34.63.222.25.nip.io/api/health | jq

for i in {1..5}; do curl -s http://asl.34.63.222.25.nip.io/api/health > /dev/null && echo "Request $i sent"; sleep 1; done
```

Check Jaeger Collector Logs for Trace Reception


Let me check what the correct service name should be:
```shell
kubectl get endpoints -n tracing
```

You should see something like this
```console
NAME               ENDPOINTS                                                    AGE
jaeger-agent       10.0.1.33:5778,10.0.1.33:5775,10.0.1.33:6832 + 1 more...     48m
jaeger-collector   10.0.1.33:14268,10.0.1.33:9411,10.0.1.33:14250 + 3 more...   48m
jaeger-query       10.0.1.33:16685,10.0.1.33:16686                              48m
```

All services are pointing to the same pod IP (10.0.1.33), which is the all-in-one pod. So the jaeger-collector service is actually working and pointing to the all-in-one pod. 

```shell
kubectl exec -n tracing pod/jaeger-5c5bdb4d75-64qz4 -- netstat -tlnp | grep 4317
```
tcp        0      0 :::4317                 :::*                    LISTEN      1/all-in-one-linux


Return the Cluster-IP of jaeger-collector if they can have connection to this svc:
```shell
kubectl exec -n model-serving deployment/hand-gesture-deployment -- python -c "import socket; print(socket.gethostbyname('jaeger-collector.tracing.svc.cluster.local'))"
```


```shell
kubectl exec -n model-serving deployment/hand-gesture-deployment -- env | grep JAEGER_COLLECTOR_HOST
```

 Port 14269 - Jaeger Admin & Metrics Service:
```shell
kubectl port-forward -n tracing pod/jaeger-5c5bdb4d75-64qz4 14269:14269 &
```

The collector IS receiving spans from your application
```shell
sleep 2 && curl -s http://localhost:14269/metrics | grep -E "(jaeger_collector_spans_received_total|jaeger_collector_spans_saved_by_svc_total)" | head -10
```

```shell
curl -s http://localhost:14269/metrics | grep -E "jaeger_collector_spans_saved_by_svc_total"
```


Check whether the Jaeger Query service reading from the same storage.
```shell
kubectl port-forward -n tracing pod/jaeger-9b5bd67cd-bdjtv 16686:16686 &
sleep 2 && curl -s "http://localhost:16686/api/services" | jq .

curl -s "http://localhost:16686/api/traces?service=hand-gesture-api&limit=5" | jq '.data | length'
```
```shell
curl -s "http://jaeger.34.63.222.25.nip.io/api/services" | jq .
```

---

## Additional Resources

- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [Jaeger Documentation](https://www.jaegertracing.io/docs/)
- [Distributed Tracing Best Practices](https://opentelemetry.io/docs/concepts/observability-primer/)
- [FastAPI Tracing Guide](https://fastapi.tiangolo.com/advanced/opentelemetry/)

---

*This guide provides a comprehensive foundation for implementing distributed tracing in FastAPI applications. Start with the basics, then gradually add more sophisticated tracing as applications grow.* 